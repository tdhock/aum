<!--
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Speed comparison}
-->

# Speed comparison

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Neuroblastoma data

Consider the neuroblastoma data. There are 3418 labeled examples. If
we consider subsets, how long does it take to compute the AUM and its
directional derivatives?

```{r}
data(neuroblastomaProcessed, package="penaltyLearning")
library(data.table)
nb.err <- data.table(neuroblastomaProcessed$errors)
nb.err[, example := paste0(profile.id, ".", chromosome)]
nb.X <- neuroblastomaProcessed$feature.mat
max.log <- if(interactive())4 else 2
(N.pred.vec <- as.integer(10^seq(1, max.log, by=0.5)))
timing.dt.list <- list()
for(N.pred in N.pred.vec){
  N.pred.names <- rownames(nb.X)[1:N.pred]
  N.diffs.dt <- aum::aum_diffs_penalty(nb.err, N.pred.names)
  pred.dt <- data.table(example=N.pred.names, pred.log.lambda=0)
  timing.df <- microbenchmark::microbenchmark(penaltyLearning={
    roc.list <- penaltyLearning::ROChange(nb.err, pred.dt, "example")
  }, aum={
    aum.list <- aum::aum(N.diffs.dt, pred.dt$pred.log.lambda)
  }, times=10)
  timing.dt.list[[paste(N.pred)]] <- with(timing.df, data.table(
    package=expr, N.pred, seconds=time/1e9))
}
(timing.dt <- do.call(rbind, timing.dt.list))

```

Below we summarize and plot these timings.

```{r}
stats.dt <- timing.dt[, .(
  q25=quantile(seconds, 0.25),
  median=median(seconds),
  q75=quantile(seconds, 0.75)
), by=.(package, N.pred)]
if(require(ggplot2)){
  gg <- ggplot()+
    geom_line(aes(
      N.pred, median, color=package),
      data=stats.dt)+
    geom_ribbon(aes(
      N.pred, ymin=q25, ymax=q75, fill=package),
      data=stats.dt,
      alpha=0.5)+
    scale_x_log10(limits=stats.dt[, c(min(N.pred), max(N.pred)*5)])+
    scale_y_log10()
  if(require(directlabels)){
    gg <- direct.label(gg, "right.polygons")
  }
}
gg
```

From the plot above we can see that both packages have similar
asymptotic time complexity. However aum is faster by
orders of magnitude (speedups shown below).

```{r}
stats.wide <- data.table::dcast(
  stats.dt, N.pred ~ package, value.var = "median")
stats.wide[, speedup := penaltyLearning/aum][]
```

## R implementation

In this section we show a base R implementation of aum.

```{r}
diffs.df <- data.frame(
  example=c(0,1,1,2,3),
  pred=c(0,0,1,0,0),
  fp_diff=c(1,1,1,0,0),
  fn_diff=c(0,0,0,-1,-1))
pred.log.lambda <- c(0,1,-1,0)
microbenchmark::microbenchmark("C++"={
  aum::aum(diffs.df, pred.log.lambda)
}, R={
  thresh.vec <- with(diffs.df, pred-pred.log.lambda[example+1])
  s.vec <- order(thresh.vec)
  sort.diffs <- data.frame(diffs.df, thresh.vec)[s.vec,]
  for(fp.or.fn in c("fp","fn")){
    ord.fun <- if(fp.or.fn=="fp")identity else rev
    fwd.or.rev <- sort.diffs[ord.fun(1:nrow(sort.diffs)),]
    fp.or.fn.diff <- fwd.or.rev[[paste0(fp.or.fn,"_diff")]]
    last.in.run <- c(diff(fwd.or.rev$thresh.vec) != 0, TRUE)
    after.or.before <-
      ifelse(fp.or.fn=="fp",1,-1)*cumsum(fp.or.fn.diff)[last.in.run]
    distribute <- function(values)with(fwd.or.rev, structure(
      values,
      names=thresh.vec[last.in.run]
    )[paste(thresh.vec)])
    out.df <- data.frame(
      before=distribute(c(0, after.or.before[-length(after.or.before)])),
      after=distribute(after.or.before))
    sort.diffs[
      paste0(fp.or.fn,"_",ord.fun(c("before","after")))
    ] <- as.list(out.df[ord.fun(1:nrow(out.df)),])
  }
  AUM.vec <- with(sort.diffs, diff(thresh.vec)*pmin(fp_before,fn_before)[-1])
  list(
    aum=sum(AUM.vec),
    deriv_mat=sapply(c("after","before"),function(b.or.a){
      s <- if(b.or.a=="before")1 else -1
      f <- function(p.or.n,suffix=b.or.a){
        sort.diffs[[paste0("f",p.or.n,"_",suffix)]]
      }
      fp <- f("p")
      fn <- f("n")
      aggregate(
        s*(pmin(fp+s*f("p","diff"),fn+s*f("n","diff"))-pmin(fp, fn)),
        list(sort.diffs$example),
        sum)$x
    }))
}, times=10)
```

It is clear that the C++ implementation is several orders of magnitude
faster.

## Synthetic data

```{r}
library(data.table)
max.N <- 1e6
(N.pred.vec <- as.integer(10^seq(1, log10(max.N), by=0.5)))
max.y.vec <- rep(c(0,1), l=max.N)
max.diffs.dt <- aum::aum_diffs_binary(max.y.vec)
set.seed(1)
max.pred.vec <- rnorm(max.N)
timing.dt.list <- list()
for(N.pred in N.pred.vec){
  print(N.pred)
  N.diffs.dt <- max.diffs.dt[1:N.pred]
  N.pred.vec <- max.pred.vec[1:N.pred]
  timing.df <- microbenchmark::microbenchmark(dt_sort={
    N.diffs.dt[order(N.pred.vec)]
  }, R_sort_radix={
    sort(N.pred.vec, method="radix")
  }, R_sort_quick={
    sort(N.pred.vec, method="quick")
  }, aum_sort={
    aum.list <- aum:::aum_sort_interface(N.diffs.dt, N.pred.vec)
  }, times=10)
  timing.dt.list[[paste(N.pred)]] <- with(timing.df, data.table(
    package=expr, N.pred, seconds=time/1e9))
}
(timing.dt <- do.call(rbind, timing.dt.list))

```

Below we summarize and plot these timings.

```{r}
stats.dt <- timing.dt[, .(
  q25=quantile(seconds, 0.25),
  median=median(seconds),
  q75=quantile(seconds, 0.75)
), by=.(package, N.pred)]
if(require(ggplot2)){
  gg <- ggplot()+
    geom_line(aes(
      N.pred, median, color=package),
      data=stats.dt)+
    geom_ribbon(aes(
      N.pred, ymin=q25, ymax=q75, fill=package),
      data=stats.dt,
      alpha=0.5)+
    scale_x_log10(limits=stats.dt[, c(min(N.pred), max(N.pred)*5)])+
    scale_y_log10()
  if(require(directlabels)){
    gg <- direct.label(gg, "right.polygons")
  }
}
gg
```

## Comparing line search speed

```{r}

X.sc <- scale(neuroblastomaProcessed$feature.mat)
keep <- apply(is.finite(X.sc), 2, all)
X.keep <- X.sc[,keep]
weight.vec <- rep(0, ncol(X.keep))
(nb.diffs <- aum::aum_diffs_penalty(nb.err, rownames(X.keep)))

if(require(atime)){
  atime.list <- atime(
    N=2^seq(1, 8, by=1),
    setup={
      step.grid <- 10^seq(-9, 1, l=N)
    }, 
    grid={
      pred.vec <- X.keep %*% weight.vec
      aum.list <- aum::aum(nb.diffs, pred.vec)
      pred.grad.vec <- rowMeans(aum.list$derivative_mat)
      weight.grad.vec <- t(X.keep) %*% pred.grad.vec
      data.table(step.size=step.grid, aum=sapply(step.grid, function(step){
        step.weight <- weight.vec-step*weight.grad.vec
        aum::aum(nb.diffs, X.keep %*% step.weight)$aum
      }))
    },
    exact.linear=aum::aum_line_search(
      nb.diffs,
      feature.mat=X.keep,
      weight.vec=weight.vec),
    exact.quadratic=aum::aum_line_search(
      nb.diffs,
      feature.mat=X.keep,
      weight.vec=weight.vec,
      maxIterations = nrow(nb.diffs)*(nrow(nb.diffs)-1)/2),
    result=TRUE,
    seconds.limit=0.1
  )
  plot(atime.list)
}
if(require(atime)){
  dcast(atime.list$measurements, N ~ expr.name, value.var="median")
}
```

The figure and table above show that the exact line search with a
linear number of iterations can be computed in about the same amount
of time as grid search with 8 points.

```{r}
if(require(atime) && require(ggplot2)){
  exact.results <- atime.list$measurements[
    expr.name!="grid", 
    result[[1]]$line_search_result[, prop.step := seq(1, .N)/.N],
    by=expr.name]
  exact.best <- exact.results[, .SD[which.min(aum)], by=expr.name]
  grid.dt <- atime.list$measurements[expr.name=="grid", {
    result[[1]][which.min(aum)]
  }, by=N]
  ggplot()+
    geom_hline(aes(
      yintercept=aum, color=expr.name),
      data=exact.best)+
    geom_point(aes(
      N, aum),
      data=grid.dt)+
    scale_x_log10(
      "Number of grid search points",
      breaks=unique(grid.dt$N))+
    theme(panel.grid.minor=element_blank())
}
```

The figure above shows that a very small number of grid points (only
4) is needed to get a better step size than the exact/approx line
search with linear number of iterations (equal to the number of
inputs/breakpoints/lines). It also shows that a modest number of grid
points, (8 or 128, depending on how close you want) is required to get
a step size which is almost as good as the exact line search with
quadratic number of iterations.

```{r}
if(require(atime) && require(ggplot2)){
  exact.quad <- exact.results[
    step.size<max(grid.dt$step.size)
  ][expr.name=="exact.quadratic"][seq(1, .N, l=1000)]
  exact.lin <- exact.results[expr.name=="exact.linear"]
  some.exact <- rbind(exact.quad, exact.lin)
  gg <- ggplot()+
    geom_line(aes(
      step.size, aum, color=expr.name, size=expr.name),
      data=some.exact)+
    geom_point(aes(
      step.size, aum),
      data=grid.dt)+
    scale_size_manual(
      values=c(exact.linear=1.5, exact.quadratic=0.5))
  if(require(ggrepel)){
    gg <- gg+
      geom_text_repel(aes(
        step.size, aum, label=N),
        data=grid.dt)+
      ggtitle("Best AUM for number of grid points shown")
  }
  gg   
}
```

The figure above shows the AUM as a function of step size, with
colored lines for two versions of the exact line search, and points
for the grid search.

```{r}
if(require(atime)){
  atimeX.list <- atime::atime(
    N=2^seq(1, 10, by=1),
    setup={
      some.X <- X.keep[1:N,]
      (some.diffs <- aum::aum_diffs_penalty(nb.err, rownames(some.X)))
      max.it <- N*(N-1)/2
    }, 
    exact.quadratic=aum::aum_line_search(
      some.diffs,
      feature.mat=some.X,
      weight.vec=weight.vec,
      maxIterations = max.it),
    result=TRUE,
    seconds.limit=0.1
  )
  plot(atimeX.list)
}
```

The plot above shows the time it takes to compute the full/quadratic
exact line search, for various data sizes `N`.

```{r}
if(require(atime) && require(ggplot2)){
  best.list <- atime::references_best(atimeX.list)
  best.refs <- best.list$ref[each.sign.rank==1]
  ref.color <- "red"
  gg <- ggplot()+
    geom_line(aes(
      N, reference, group=fun.name),
      color=ref.color,
      data=best.refs)+
    geom_ribbon(aes(
      N, ymin=min, ymax=max),
      alpha=0.5,
      data=data.table(unit="seconds", atimeX.list$meas))+
    geom_line(aes(
      N, empirical),
      linewidth=1,
      data=best.list$meas)+
    scale_x_log10()+
    scale_y_log10("median line, min/max band")+
    facet_grid(unit ~ ., scales="free")+
    theme_bw()
  if(require(directlabels)){
    gg+
      directlabels::geom_dl(aes(
        N, reference, label=fun.name),
        data=best.refs,
        color=ref.color,
        method="bottom.polygons")
  }else{
    gg
  }
}
```

The figure above shows reference lines in red, which clearly show the
quadratic time/space complexity of computing the full exact line
search.

```{r}
if(require(atime) && require(nc) && require(ggplot2) && require(directlabels)){
  (N.step.wide <- atimeX.list$measurements[, {
    ls.list <- result[[1]]
    res.dt <- ls.list$line_search_result
    s <- res.dt$step.size
    N.lines <- nrow(ls.list$line_search_input)
    data.table(
      min.step=s[2],
      linear.step=s[N.lines],
      best.step=res.dt[which.min(aum), step.size],
      max.step=max(s))
  }, by=N])
  N.step.tall <- capture_melt_single(
    N.step.wide[max.step>0],
    step.type=".*?",
    "[.]step",
    value.name="step.size")
  gg <- ggplot()+
    geom_line(aes(
      N, step.size, color=step.type),
      data=N.step.tall)+
    scale_y_log10()+
    scale_x_log10(limits=c(NA,max(N.step.tall$N)*2))
  directlabels::direct.label(gg,"right.polygons")
}
```

The figure above shows various step sizes from the exact line search,
as a function of data size `N`. It is clear that, in general, the
larger data sizes result in smaller step sizes.
