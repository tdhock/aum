<!--
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Line search}
-->

# Accuracy comparison

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width=14  
)
```

## Comparing line search with different number of max iterations

The code below compares learning with different values of the
maxIterations parameter.

```{r}
if(requireNamespace("penaltyLearning")){

  data(neuroblastomaProcessed, package="penaltyLearning", envir=environment())
  nb.err <- with(neuroblastomaProcessed$errors, data.frame(
    example=paste0(profile.id, ".", chromosome),
    min.lambda,
    max.lambda,
    fp, fn))
  library(data.table)
  signal.features <- neuroblastomaProcessed$feature.mat[,c("log2.n","log.hall")]
  n.noise <- 20
  set.seed(1)
  noise.features <- matrix(
    rnorm(n.noise*nrow(signal.features)),
    nrow(signal.features), n.noise)
  X.sc <- scale(cbind(signal.features, noise.features))
  keep <- apply(is.finite(X.sc), 2, all)
  X.keep <- X.sc[,keep]
  n.folds <- 3
  uniq.folds <- 1:n.folds
  fold.vec <- sample(rep(uniq.folds, l=nrow(X.keep)))
  loss.dt.list <- list()
  timing.dt.list <- list()
  for(valid.fold in uniq.folds){
    index.list <- list(
      subtrain=fold.vec!=valid.fold,
      validation=fold.vec==valid.fold)
    diff.list <- lapply(index.list, function(set.i){
      aum::aum_diffs_penalty(nb.err, rownames(X.keep)[set.i])
    })
    for(maxIterations in 10^seq(2, 6)){
      weight.vec <- rep(0, ncol(X.keep))
      improvement <- old.aum <- Inf
      step.number <- 0
      cat(sprintf("fold=%d maxIt=%d\n", valid.fold, maxIterations))
      seconds <- system.time({
        while(improvement > 1e-4){
          step.number <- step.number+1
          valid.list <- aum::aum(
            diff.list$validation,
            X.keep[index.list$validation,] %*% weight.vec)
          nb.weight.search <- aum::aum_line_search(
            diff.list$subtrain,
            maxIterations=maxIterations,
            feature.mat=X.keep[index.list$subtrain,],
            weight.vec=weight.vec)
          loss.dt.list[[paste(
            valid.fold, step.number, maxIterations
          )]] <- data.table(
            valid.fold, step.number, maxIterations,
            set=c("subtrain", "validation"),
            aum=c(nb.weight.search$aum, valid.list$aum))
          exact.dt <- data.table(nb.weight.search$line_search_result)
          exact.dt[, kink := .I/.N]
          best.row <- exact.dt[which.min(aum)]
          improvement <- old.aum-best.row$aum
          old.aum <- best.row$aum
          weight.vec <- weight.vec-
            best.row$step.size*nb.weight.search$gradient_weight
        }
      })[["elapsed"]]
      timing.dt.list[[paste(valid.fold, maxIterations)]] <- data.table(
        valid.fold, maxIterations, seconds)
    }
  }
  (timing.dt <- do.call(rbind, timing.dt.list))
  print(dcast(timing.dt, maxIterations ~ valid.fold, value.var="seconds"))

}
```

The table above shows the total number of seconds of computation time
for running gradient descent, for different values of max iterations
of the exact line search (rows), and for each cross-validation fold
(columns). It is clear that the minimal computation time is achieved
by using an intermediate value for max iterations.

```{r}
if(requireNamespace("penaltyLearning")){
  
  loss.dt <- do.call(
    rbind, loss.dt.list)[
    , max.step := max(step.number), by=.(maxIterations, valid.fold)][
    , min.step := min(max.step), by=maxIterations
    ][step.number<=min.step]
  min.dt <- loss.dt[, .SD[which.min(aum)], by=.(
    valid.fold, maxIterations, set)]
  if(requireNamespace("ggplot2")){
    g <- ggplot2::ggplot()+
      ggplot2::facet_grid(
        valid.fold ~ maxIterations,
        labeller=ggplot2::label_both,
        scales="free")+
      ggplot2::geom_line(ggplot2::aes(
        step.number, aum, color=set),
        data=loss.dt)+
      ggplot2::geom_point(ggplot2::aes(
        step.number, aum, color=set),
        shape=1,
        data=min.dt)+
      ggplot2::scale_y_log10()
    print(g)
  }

}
```

The plot above has a panel for each number of max iterations of the
exact line search (from left to right) and for each cross-validation
fold (from top to bottom). The expected loss curves are evident:
subtrain always decreasing, validation U shaped.

```{r}
if(requireNamespace("penaltyLearning")){

  mean.dt <- loss.dt[, .(
    mean.aum=mean(aum)
  ), by=.(step.number,maxIterations,set)]
  (selected.dt <- mean.dt[set=="validation"][which.min(mean.aum)])
  if(requireNamespace("ggplot2")){
    g <- ggplot2::ggplot()+
      ggplot2::facet_grid(
        . ~ maxIterations,
        labeller=ggplot2::label_both,
        scales="free")+
      ggplot2::geom_line(ggplot2::aes(
        step.number, mean.aum, color=set),
        data=mean.dt)+
      ggplot2::geom_point(ggplot2::aes(
        step.number, mean.aum, color=set),
        shape=1,
        data=selected.dt)+
      ggplot2::geom_hline(ggplot2::aes(
        yintercept=mean.aum, color=set),
        data=selected.dt[, .(mean.aum, set)])+
      ggplot2::scale_y_log10()
    print(g)
  }

}

```

The plot above shows the mean AUM over cross-validation folds, for
each step (X axis) and for each value of max iterations in the exact
line search (panels from left to right). It is clear that the min
validation error is about the same for each value of max iterations.

## linear model cv

```{r}
## learn a model for a real changepoint data set.
if(requireNamespace("penaltyLearning")){

  library(data.table)
  data(neuroblastomaProcessed, package="penaltyLearning", envir=environment())
  nb.err <- with(neuroblastomaProcessed$errors, data.table(
    example=paste0(profile.id, ".", chromosome),
    min.log.lambda,
    max.log.lambda, 
    min.lambda, 
    max.lambda, 
    fp, fn, errors=fp+fn))
  signal.features <- neuroblastomaProcessed$feature.mat[,c("log2.n","log.hall")]
  n.folds <- 3
  uniq.folds <- 1:n.folds
  fold.vec <- sample(rep(uniq.folds, l=nrow(signal.features)))
  test.fold <- 1
  index.list <- list(
    train=fold.vec!=test.fold,
    test=fold.vec==test.fold)
  n.noise <- 40
  set.seed(1)
  noise.features <- matrix(
    rnorm(n.noise*nrow(signal.features)),
    nrow(signal.features), n.noise)
  input.features <- cbind(-54, 1, signal.features, noise.features)
  set.data <- lapply(index.list, function(is.set){
    L <- list(features=input.features[is.set,])
    for(denominator in c("count","rate")){
      L[[paste0(denominator, ".diffs")]] <- aum::aum_diffs_penalty(
        nb.err, rownames(L$features), denominator=denominator)
    }
    L
  })
  model <- with(set.data$train, aum::aum_linear_model_cv(
    features, count.diffs))
  plot(model)

  ## verify that the predictions are the same using either scaled or
  ## original features.
  with(set.data$train, rbind(
    predict.on.inputs=t(head(
      predict(model, features))),
    scale.then.predict=t(head(
      scale(features)[,model$keep] %*% model$weight.vec +
        model$intercept))))

  ## verify that the learned intercept results in min errors, at thresh=0.
  train.list <- with(set.data$train, aum::aum(
    count.diffs, predict(model, features)))
  plot(fp_before+fn_before ~ thresh, train.list$total_error)

  ## use rates instead of counts for computing AUM.
  set.seed(1)
  rate.model <- with(set.data$train, aum::aum_linear_model_cv(
    features, rate.diffs))
  plot(rate.model)

  ## alternative visualization including error bands and min loss.
  if(requireNamespace("ggplot2")){
    g <- ggplot2::ggplot()+
      ggplot2::geom_ribbon(ggplot2::aes(
        step.number, ymin=aum_mean-aum_sd, ymax=aum_mean+aum_sd, fill=set),
        alpha=0.5,
        data=rate.model$set.loss)+
      ggplot2::geom_line(ggplot2::aes(
        step.number, aum_mean, color=set),
        data=rate.model$set.loss)+
      ggplot2::geom_point(ggplot2::aes(
        step.number, aum_mean, color=set),
        data=rate.model$set.loss[, .SD[which.min(aum_mean)], by=set])+
      ggplot2::scale_y_log10()
    print(g)
  }
  
  ## alternative visualization showing each fold.
  if(requireNamespace("ggplot2")){
    g <- ggplot2::ggplot()+
      ggplot2::geom_line(ggplot2::aes(
        step.number, aum, color=set),
        data=rate.model$fold.loss)+
      ggplot2::geom_point(ggplot2::aes(
        step.number, aum, color=set),
        data=rate.model$fold.loss[
        , .SD[which.min(aum)], by=.(valid.fold, set)])+
      ggplot2::scale_y_log10()+
      ggplot2::facet_grid(. ~ valid.fold, labeller = "label_both")
    print(g)
  }

  ## compute test ROC curves, and compare against L1 regularized
  ## linear model with squared hinge loss.
  target.dt <- penaltyLearning::targetIntervals(nb.err, "example")
  target.mat <- target.dt[
    rownames(set.data$train$features),
    cbind(min.log.lambda, max.log.lambda),
    on="example"]
  named.features <- as.matrix(data.frame(input.features))
  ircv <- penaltyLearning::IntervalRegressionCV(
    named.features[rownames(set.data$train$features),], target.mat)
  pred.list <- with(set.data$test, list(
    IRCV=-predict(ircv, named.features[rownames(features),]),
    AUM.count=predict(model, features),
    AUM.rate=predict(rate.model, features),
    zero=rep(0, nrow(features))))
  roc.dt <- data.table(pred.name=names(pred.list))[, {
    aum::aum(set.data$test$rate.diffs, pred.list[[pred.name]])$total_error
  }, by=pred.name][, tp_before := 1-fn_before][]
  setkey(roc.dt, pred.name, thresh)
  if(requireNamespace("ggplot2")){
    pred.dt <- roc.dt[0 < thresh, .SD[1], by=pred.name]
    g <- ggplot2::ggplot()+
      ggplot2::ggtitle("Test set ROC curves, dot for predicted threshold")+
      ggplot2::geom_path(ggplot2::aes(
        fp_before, tp_before, color=pred.name),
        data=roc.dt)+
      ggplot2::geom_point(ggplot2::aes(
        fp_before, tp_before, color=pred.name),
        shape=21,
        fill="white",
        data=pred.dt)+
      ggplot2::coord_equal(xlim=c(0,0.25), ylim=c(0.75,1))+
      ggplot2::xlab("False Positive Rate")+
      ggplot2::ylab("True Positive Rate")
    print(g)
  }
  ## first and last row of each pred.
  roc.dt[, .SD[c(1,.N)], by=pred.name]

  ## visualize area under min.
  if(requireNamespace("nc")){
    roc.dt[, min_before := pmin(fp_before, fn_before)]
    roc.tall <- nc::capture_melt_single(
      roc.dt, 
      error.type="fp|fn|min",
      "_before",
      value.name="error.value")
    err.sizes <- c(
      fp=3,
      fn=2,
      min=1)
    err.colors <- c(
      fp="red",
      fn="deepskyblue",
      min="black")
    if(requireNamespace("ggplot2")){
      g <- ggplot2::ggplot()+
        ggplot2::ggtitle("Train set AUM in green")+
        ggplot2::theme_bw()+
        ggplot2::geom_step(ggplot2::aes(
          thresh, error.value, color=error.type, linewidth=error.type),
          data=roc.tall)+
        ggplot2::geom_polygon(ggplot2::aes(
          thresh, min_before),
          fill="green",
          data=roc.dt)+
        ggplot2::facet_grid(pred.name ~ ., labeller="label_both")+
        ggplot2::xlab("Constant added to predicted values")+
        ggplot2::ylab("Error rate")+
        ggplot2::scale_color_manual(values=err.colors)+
        ggplot2::scale_size_manual(values=err.sizes)
      print(g)
    }
  }

}

```
